{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d3d39ab",
   "metadata": {
    "id": "3d3d39ab"
   },
   "source": [
    "# Intelligent App with Google Generative AI and Neo4j\n",
    "In this notebook, let's explore how to leverage Google GenAI to build and consume a knowledge graph in Neo4j.\n",
    "\n",
    "This notebook parses data from a public [corpus of Resumes / Curriculum Vitae](https://github.com/florex/resume_corpus) using Google Vertex AI Generative AI's `text-bison` model. The model will be prompted to recognise and extract entities and relationships. We will then generate Neo4j Cypher queries using them and write the data to a Neo4j database.\n",
    "We will again use a `text-bison` model and prompt it to convert questions in english to Cypher - Neo4j's query language, which can be used for data retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17a9328-f17d-48ae-b72e-c333fb867eb0",
   "metadata": {
    "id": "f17a9328-f17d-48ae-b72e-c333fb867eb0",
    "tags": []
   },
   "source": [
    "## Setup\n",
    "First off, check that the Python environment you installed in the readme is running this notebook. Make sure you select the `py38` kernel in the top right of this notebook. You should see a 3.8 version when you run this command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80b97b17-4ec5-447a-88d0-df6faaf662fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "80b97b17-4ec5-447a-88d0-df6faaf662fe",
    "outputId": "24d9c962-7a2e-4347-d78e-fdea74fbf36c",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.10.13 | packaged by conda-forge | (main, Oct 26 2023, 18:07:37) [GCC 12.3.0]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xKUrHD1hXiMm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xKUrHD1hXiMm",
    "outputId": "b421f483-d03f-4143-edaf-514112953f05"
   },
   "outputs": [],
   "source": [
    "! git clone https://github.com/neo4j-partners/intelligent-app-google-generativeai-neo4j.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eb2fda-70c8-4733-ac6f-2678e5cbff47",
   "metadata": {
    "id": "87eb2fda-70c8-4733-ac6f-2678e5cbff47"
   },
   "source": [
    "Next we need to install some libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43720d2e-05cd-49de-bbb9-15c0b10d768b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "43720d2e-05cd-49de-bbb9-15c0b10d768b",
    "outputId": "63beb357-beea-4ef5-8f4b-d57aa5d87334"
   },
   "outputs": [],
   "source": [
    "%pip install --user \"google-cloud-aiplatform>=1.25.0\" --upgrade\n",
    "%pip install --user \"google-cloud-aiplatform[pipelines]>=1.25.0\"\n",
    "%pip install --user \"langchain==0.0.198\"\n",
    "%pip install --user neo4j\n",
    "%pip install --user pydantic\n",
    "%pip install --user gradio\n",
    "%pip install --user IProgress\n",
    "%pip install --user tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f91036-7018-465f-b4af-8d5523e7ed3a",
   "metadata": {
    "id": "42f91036-7018-465f-b4af-8d5523e7ed3a"
   },
   "source": [
    "Now restart the kernel.  That will allow the Python evironment to import the new packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44ccbf1-28bf-48cc-969f-30932ad9bd95",
   "metadata": {
    "id": "a44ccbf1-28bf-48cc-969f-30932ad9bd95"
   },
   "source": [
    "Provide your `Project ID` (**NOT** Project Name) & `location` in the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad823ee-84ca-4de3-9b29-4b324ac5b9ae",
   "metadata": {
    "id": "4ad823ee-84ca-4de3-9b29-4b324ac5b9ae"
   },
   "outputs": [],
   "source": [
    "# Note, you will need to set your project_id\n",
    "project_id = 'workshop-trvlk'\n",
    "location = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c5a70c-aee1-462b-953a-4c87a524a111",
   "metadata": {
    "id": "42c5a70c-aee1-462b-953a-4c87a524a111"
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "vertexai.init(project=project_id, location='us-central1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e43bb3c",
   "metadata": {
    "id": "4e43bb3c"
   },
   "source": [
    "## Prompt Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72960046",
   "metadata": {
    "id": "72960046"
   },
   "source": [
    "In the upcoming sections, we will extract knowledge adhering to the following schema. This is a very Simplified schema to denote a Resume. Normally, you will have Domain Experts who come up with an ideal Ontology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2599f51b-aa46-471e-be3f-2544124dae3d",
   "metadata": {
    "id": "2599f51b-aa46-471e-be3f-2544124dae3d"
   },
   "source": [
    "![schema.png](attachment:4bb6059e-7375-4dd2-99cd-f3142706d6e9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7781a12b",
   "metadata": {
    "id": "7781a12b"
   },
   "source": [
    "To achieve our Extraction goal as per the schema, I am going to chain a series of prompts, each focused on only one task - to extract a specific entity. By this way, you can avoid Token limitations. Also, the quality of extraction will be good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7921fa85",
   "metadata": {
    "id": "7921fa85"
   },
   "outputs": [],
   "source": [
    "person_prompt_tpl=\"\"\"From the Resume text for a job aspirant below, extract Entities strictly as instructed below\n",
    "1. First, look for the Person Entity type in the text and extract the needed information defined below:\n",
    "   `id` property of each entity must be alphanumeric and must be unique among the entities. You will be referring this property to define the relationship between entities. NEVER create new entity types that aren't mentioned below. Document must be summarized and stored inside Person entity under `description` property\n",
    "    Entity Types:\n",
    "    label:'Person',id:string,role:string,description:string //Person Node\n",
    "2. Description property should be a crisp text summary and MUST NOT be more than 100 characters\n",
    "3. If you cannot find any information on the entities & relationships above, it is okay to return empty value. DO NOT create fictious data\n",
    "4. Do NOT create duplicate entities\n",
    "5. Restrict yourself to extract only Person information. No Position, Company, Education or Skill information should be focussed.\n",
    "6. NEVER Impute missing values\n",
    "Example Output JSON:\n",
    "{\"entities\": [{\"label\":\"Person\",\"id\":\"person1\",\"role\":\"Prompt Developer\",\"description\":\"Prompt Developer with more than 30 years of LLM experience\"}]}\n",
    "\n",
    "Question: Now, extract the Person for the text below -\n",
    "$ctext\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d903cb98-ce84-43bb-b860-71a394604a52",
   "metadata": {
    "id": "d903cb98-ce84-43bb-b860-71a394604a52"
   },
   "outputs": [],
   "source": [
    "postion_prompt_tpl=\"\"\"From the Resume text for a job aspirant below, extract Entities & relationships strictly as instructed below\n",
    "1. First, look for Position & Company types in the text and extract information in comma-separated format. Position Entity denotes the Person's previous or current job. Company node is the Company where they held that position.\n",
    "   `id` property of each entity must be alphanumeric and must be unique among the entities. You will be referring this property to define the relationship between entities. NEVER create new entity types that aren't mentioned below. You will have to generate as many entities as needed as per the types below:\n",
    "    Entity Types:\n",
    "    label:'Position',id:string,title:string,location:string,startDate:string,endDate:string,url:string //Position Node\n",
    "    label:'Company',id:string,name:string //Company Node\n",
    "2. Next generate each relationships as triples of head, relationship and tail. To refer the head and tail entity, use their respective `id` property. NEVER create new Relationship types that aren't mentioned below:\n",
    "    Relationship definition:\n",
    "    position|AT_COMPANY|company //Ensure this is a string in the generated output\n",
    "3. If you cannot find any information on the entities & relationships above, it is okay to return empty value. DO NOT create fictious data\n",
    "4. Do NOT create duplicate entities.\n",
    "5. No Education or Skill information should be extracted.\n",
    "6. DO NOT MISS out any Position or Company related information\n",
    "7. NEVER Impute missing values\n",
    " Example Output JSON:\n",
    "{\"entities\": [{\"label\":\"Position\",\"id\":\"position1\",\"title\":\"Software Engineer\",\"location\":\"Singapore\",startDate:\"2021-01-01\",endDate:\"present\"},{\"label\":\"Position\",\"id\":\"position2\",\"title\":\"Senior Software Engineer\",\"location\":\"Mars\",startDate:\"2020-01-01\",endDate:\"2020-12-31\"},{label:\"Company\",id:\"company1\",name:\"Neo4j Singapore Pte Ltd\"},{\"label\":\"Company\",\"id\":\"company2\",\"name\":\"Neo4j Mars Inc\"}],\"relationships\": [\"position1|AT_COMPANY|company1\",\"position2|AT_COMPANY|company2\"]}\n",
    "\n",
    "Question: Now, extract entities & relationships as mentioned above for the text below -\n",
    "$ctext\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdf7091-20ff-4537-9aa8-b96a9325b2a6",
   "metadata": {
    "id": "ffdf7091-20ff-4537-9aa8-b96a9325b2a6"
   },
   "outputs": [],
   "source": [
    "skill_prompt_tpl=\"\"\"From the Resume text below, extract Entities strictly as instructed below\n",
    "1. Look for prominent Skill Entities in the text. The`id` property of each entity must be alphanumeric and must be unique among the entities. NEVER create new entity types that aren't mentioned below:\n",
    "    Entity Definition:\n",
    "    label:'Skill',id:string,name:string,level:string //Skill Node\n",
    "2. NEVER Impute missing values\n",
    "3. If you do not find any level information: assume it as `expert` if the experience in that skill is more than 5 years, `intermediate` for 2-5 years and `beginner` otherwise.\n",
    "Example Output Format:\n",
    "{\"entities\": [{\"label\":\"Skill\",\"id\":\"skill1\",\"name\":\"Neo4j\",\"level\":\"expert\"},{\"label\":\"Skill\",\"id\":\"skill2\",\"name\":\"Pytorch\",\"level\":\"expert\"}]}\n",
    "\n",
    "Question: Now, extract entities as mentioned above for the text below -\n",
    "$ctext\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e188d63a-1a13-4363-8917-1af3123cbda1",
   "metadata": {
    "id": "e188d63a-1a13-4363-8917-1af3123cbda1"
   },
   "outputs": [],
   "source": [
    "edu_prompt_tpl=\"\"\"From the Resume text for a job aspirant below, extract Entities strictly as instructed below\n",
    "1. Look for Education entity type and generate the information defined below:\n",
    "   `id` property of each entity must be alphanumeric and must be unique among the entities. You will be referring this property to define the relationship between entities. NEVER create other entity types that aren't mentioned below. You will have to generate as many entities as needed as per the types below:\n",
    "    Entity Definition:\n",
    "    label:'Education',id:string,degree:string,university:string,graduationDate:string,score:string,url:string //Education Node\n",
    "2. If you cannot find any information on the entities above, it is okay to return empty value. DO NOT create fictious data\n",
    "3. Do NOT create duplicate entities or properties\n",
    "4. Strictly extract only Education. No Skill or other Entities should be extracted\n",
    "5. DO NOT MISS out any Education related entity\n",
    "6. NEVER Impute missing values\n",
    "Output JSON (Strict):\n",
    "{\"entities\": [{\"label\":\"Education\",\"id\":\"education1\",\"degree\":\"Bachelor of Science\",\"graduationDate\":\"May 2022\",\"score\":\"0.0\"}]}\n",
    "\n",
    "Question: Now, extract Education information as mentioned above for the text below -\n",
    "$ctext\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Tc-dCi-uRLJq",
   "metadata": {
    "id": "Tc-dCi-uRLJq"
   },
   "outputs": [],
   "source": [
    "# Authenticate with Google Cloud credentials\n",
    "from google.colab import auth as google_auth\n",
    "google_auth.authenticate_user()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605ad98c",
   "metadata": {
    "id": "605ad98c"
   },
   "source": [
    "This is a helper function to talk to the LLM with our prompt and text input. We will use the `text-bison` base model. In your usecase, you might need to tune it. VertexAI provides an elegant way to finetune it. The weights will be staying within your tenant and the base model is frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1148d87e",
   "metadata": {
    "id": "1148d87e"
   },
   "outputs": [],
   "source": [
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "\n",
    "def run_text_model(\n",
    "    project_id: str,\n",
    "    model_name: str,\n",
    "    temperature: float,\n",
    "    max_decode_steps: int,\n",
    "    top_p: float,\n",
    "    top_k: int,\n",
    "    prompt: str,\n",
    "    location: str = \"us-central1\",\n",
    "    tuned_model_name: str = \"\",\n",
    "    ) :\n",
    "    \"\"\"Text Completion Use a Large Language Model.\"\"\"\n",
    "    vertexai.init(project=project_id, location=location)\n",
    "    model = TextGenerationModel.from_pretrained(model_name)\n",
    "    if tuned_model_name:\n",
    "      model = model.get_tuned_model(tuned_model_name)\n",
    "    response = model.predict(\n",
    "        prompt,\n",
    "        temperature=temperature,\n",
    "        max_output_tokens=max_decode_steps,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbfd725",
   "metadata": {
    "id": "dcbfd725"
   },
   "outputs": [],
   "source": [
    "def extract_entities_relationships(prompt, tuned_model_name):\n",
    "    try:\n",
    "        res = run_text_model(project_id, \"text-bison@001\", 0, 1024, 0.8, 40, prompt, location, tuned_model_name)\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf22363-093f-4dd3-b882-7e10f0d5e6e4",
   "metadata": {
    "id": "fcf22363-093f-4dd3-b882-7e10f0d5e6e4"
   },
   "source": [
    "Now, let's run our extraction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c9cfd6-93db-46a5-bcdc-46abacde65c0",
   "metadata": {
    "id": "95c9cfd6-93db-46a5-bcdc-46abacde65c0"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    return re.sub(r'[^\\x00-\\x7F]+',' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6JKeHYgb4op-",
   "metadata": {
    "id": "6JKeHYgb4op-"
   },
   "outputs": [],
   "source": [
    "from string import Template\n",
    "import json\n",
    "\n",
    "sample_que = \"\"\"Developer <span class=\"hl\">Developer</span> Developer - TATA CONSULTANTCY SERVICE Batavia, OH Relevant course work† Database Systems, Database Administration, Database Security & Auditing, Computer Security,Computer Networks, Programming & Software Development, IT, Information Security Concept & Admin,† IT System Acquisition & Integration, Advanced Web Development, and Ethical Hacking: Network Security & Pen Testing. Work Experience Developer TATA CONSULTANTCY SERVICE June 2016 to Present MRM (Government of ME, RI, MS) Developer†††† Working with various technologies such as Java, JSP, JSF, DB2(SQL), LDAP, BIRT report, Jazz version control, Squirrel SQL client, Hibernate, CSS, Linux, and Windows. Work as part of a team that provide support to enterprise applications. Perform miscellaneous support activities as requested by Management. Perform in-depth research and identify sources of production issues.†† SPLUNK Developer† Supporting the Splunk Operational environment for Business Solutions Unit aiming to support overall business infrastructure. Developing Splunk Queries to generate the report, monitoring, and analyzing machine generated big data for server that has been using for onsite and offshore team. Working with Splunk' premium apps such as ITSI, creating services, KPI, and glass tables. Developing app with custom dashboard with front- end ability and advanced XML to serve Business Solution unit' needs. Had in-house app presented at Splunk's .Conf Conference (2016). Help planning, prioritizing and executing development activities. Developer ( front end) intern TOMORROW PICTURES INC - Atlanta, GA April 2015 to January 2016 Assist web development team with multiple front end web technologies and involved in web technologies such as Node.js, express, json, gulp.js, jade, sass, html5, css3, bootstrap, WordPress.†Testing (manually), version control (GitHub), mock up design and ideas Education MASTER OF SCIENCE IN INFORMATION TECHNOLOGY in INFOTMATION TECHNOLOGY KENNESAW STATE UNIVERSITY - Kennesaw, GA August 2012 to May 2015 MASTER OF BUSINESS ADMINISTRATION in INTERNATIONAL BUSINESS AMERICAN INTER CONTINENTAL UNIVERSITY ATLANTA November 2003 to December 2005 BACHELOR OF ARTS in PUBLIC RELATIONS THE UNIVERSITY OF THAI CHAMBER OF COMMERCE - BANGKOK, TH June 1997 to May 2001 Skills Db2 (2 years), front end (2 years), Java (2 years), Linux (2 years), Splunk (2 years), SQL (3 years) Certifications/Licenses Splunk Certified Power User V6.3 August 2016 to Present CERT-112626 Splunk Certified Power User V6.x May 2017 to Present CERT-168138 Splunk Certified User V6.x May 2017 to Present CERT -181476 Driver's License Additional Information Skills† ∑††††SQL, PL/SQL, Knowledge of Data Modeling, Experience on Oracle database/RDBMS.† ∑††††††††Database experience on Oracle, DB2, SQL Sever, MongoDB, and MySQL.† ∑††††††††Knowledge of tools including Splunk, tableau, and wireshark.† ∑††††††††Knowledge of SCRUM/AGILE and WATERFALL methodologies.† ∑††††††††Web technology included: HTML5, CSS3, XML, JSON, JavaScript, node.js, NPM, GIT, express.js, jQuery, Angular, Bootstrap, and Restful API.† ∑††††††††Working Knowledge in JAVA, J2EE, and PHP.† Operating system Experience included: Windows, Mac OS, Linux (Ubuntu, Mint, Kali)††\"\"\"\n",
    "prompts = [person_prompt_tpl, postion_prompt_tpl, skill_prompt_tpl, edu_prompt_tpl]\n",
    "results = {\"entities\": [], \"relationships\": []}\n",
    "\n",
    "for p in prompts:\n",
    "    _prompt = Template(p).substitute(ctext=clean_text(sample_que))\n",
    "    _extraction = extract_entities_relationships(_prompt, '')\n",
    "    if 'Answer:\\n' in _extraction:\n",
    "        _extraction = _extraction.split('Answer:\\n ')[1]\n",
    "    if _extraction.strip() == '':\n",
    "        continue\n",
    "    try:\n",
    "        _extraction = json.loads(_extraction.replace(\"\\'\", \"'\").replace('`', ''))\n",
    "    except json.JSONDecodeError:\n",
    "        # print(_extraction)\n",
    "        #Temp hack to ignore Skills cut off by token limitation\n",
    "        _extraction = _extraction[:_extraction.rfind(\"}\")+1] + ']}'\n",
    "        _extraction = json.loads(_extraction.replace(\"\\'\", \"'\"))\n",
    "    results[\"entities\"].extend(_extraction[\"entities\"])\n",
    "    if \"relationships\" in _extraction:\n",
    "        results[\"relationships\"].extend(_extraction[\"relationships\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4b37bf-f355-4571-95e1-f307a65483b6",
   "metadata": {
    "id": "df4b37bf-f355-4571-95e1-f307a65483b6"
   },
   "outputs": [],
   "source": [
    "person_id = results[\"entities\"][0][\"id\"]\n",
    "for e in results[\"entities\"][1:]:\n",
    "    if e['label'] == 'Position':\n",
    "        results[\"relationships\"].append(f\"{person_id}|HAS_POSITION|{e['id']}\")\n",
    "    if e['label'] == 'Skill':\n",
    "        results[\"relationships\"].append(f\"{person_id}|HAS_SKILL|{e['id']}\")\n",
    "    if e['label'] == 'Education':\n",
    "        results[\"relationships\"].append(f\"{person_id}|HAS_EDUCATION|{e['id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4720e425-17fe-41b1-999a-076bbf7a7470",
   "metadata": {
    "id": "4720e425-17fe-41b1-999a-076bbf7a7470"
   },
   "source": [
    "The extracted entities & relationships will look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a07624b-a1bb-493e-820a-144841b8a6ac",
   "metadata": {
    "id": "4a07624b-a1bb-493e-820a-144841b8a6ac",
    "outputId": "4cc95670-5f8d-4872-d613-dedc78dfe4a2"
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feb2a60",
   "metadata": {
    "id": "8feb2a60"
   },
   "source": [
    "## Data Ingestion Cypher Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b96efc5",
   "metadata": {
    "id": "0b96efc5"
   },
   "source": [
    "The entities and relationships we got from the LLM have to be transformed to Cypher so we can write them into Neo4j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084047d0",
   "metadata": {
    "id": "084047d0"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "\n",
    "def get_prop_str(prop_dict, _id):\n",
    "    s = []\n",
    "    for key, val in prop_dict.items():\n",
    "      if key != 'label' and key != 'id':\n",
    "         s.append(_id+\".\"+key+' = \"'+str(val).replace('\\\"', '\"').replace('\"', '\\\"')+'\"')\n",
    "    return ' ON CREATE SET ' + ','.join(s)\n",
    "\n",
    "def get_cypher_compliant_var(_id):\n",
    "    s = \"_\"+ re.sub(r'[\\W_]', '', _id).lower() #avoid numbers appearing as firstchar; replace spaces\n",
    "    return s[:20] #restrict variable size\n",
    "\n",
    "def generate_cypher(in_json):\n",
    "    e_map = {}\n",
    "    e_stmt = []\n",
    "    r_stmt = []\n",
    "    e_stmt_tpl = Template(\"($id:$label{id:'$key'})\")\n",
    "    r_stmt_tpl = Template(\"\"\"\n",
    "      MATCH $src\n",
    "      MATCH $tgt\n",
    "      MERGE ($src_id)-[:$rel]->($tgt_id)\n",
    "    \"\"\")\n",
    "    for obj in in_json:\n",
    "      for j in obj['entities']:\n",
    "          props = ''\n",
    "          label = j['label']\n",
    "          id = ''\n",
    "          if label == 'Person':\n",
    "            id = 'p'+str(time.time_ns())\n",
    "          elif label == 'Position':\n",
    "            id = 'j'+str(time.time_ns())\n",
    "          elif label == 'Education':\n",
    "            id = 'e'+str(time.time_ns())\n",
    "          else:\n",
    "                id = get_cypher_compliant_var(j['name'])\n",
    "          if label in ['Person', 'Position', 'Education', 'Skill', 'Company']:\n",
    "            varname = get_cypher_compliant_var(j['id'])\n",
    "            stmt = e_stmt_tpl.substitute(id=varname, label=label, key=id)\n",
    "            e_map[varname] = stmt\n",
    "            e_stmt.append('MERGE '+ stmt + get_prop_str(j, varname))\n",
    "\n",
    "      for st in obj['relationships']:\n",
    "          rels = st.split(\"|\")\n",
    "          src_id = get_cypher_compliant_var(rels[0].strip())\n",
    "          rel = rels[1].strip()\n",
    "          if rel in ['HAS_SKILL', 'HAS_EDUCATION', 'AT_COMPANY', 'HAS_POSITION']: #we ignore other relationships\n",
    "            tgt_id = get_cypher_compliant_var(rels[2].strip())\n",
    "            stmt = r_stmt_tpl.substitute(\n",
    "              src_id=src_id, tgt_id=tgt_id, src=e_map[src_id], tgt=e_map[tgt_id], rel=rel)\n",
    "            r_stmt.append(stmt)\n",
    "\n",
    "    return e_stmt, r_stmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec143b14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ec143b14",
    "outputId": "6f19e6bf-8e80-4918-d2ca-e8fa68d7f9d0"
   },
   "outputs": [],
   "source": [
    "ent_cyp, rel_cyp = generate_cypher([results])\n",
    "\n",
    "print(ent_cyp, rel_cyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c69170",
   "metadata": {
    "id": "54c69170",
    "tags": []
   },
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f06013-a653-43cf-be7c-de2888e621f7",
   "metadata": {
    "id": "00f06013-a653-43cf-be7c-de2888e621f7"
   },
   "source": [
    "You will need a Neo4j AuraDS Pro instance.  You can deploy that on Google Cloud Marketplace [here](https://console.cloud.google.com/marketplace/product/endpoints/prod.n4gcp.neo4j.io).\n",
    "\n",
    "With that complete, you'll need to install the Neo4j library and set up your database connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e621c199-533a-4503-baef-200c5adcd8ad",
   "metadata": {
    "id": "e621c199-533a-4503-baef-200c5adcd8ad"
   },
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecea5ff",
   "metadata": {
    "id": "0ecea5ff"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "# You will need to change these variables\n",
    "connectionUrl = \"bolt://3.238.140.38:7687\"\n",
    "username = \"neo4j\"\n",
    "password = \"streams-waist-splashes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbfa6e8",
   "metadata": {
    "id": "ddbfa6e8"
   },
   "outputs": [],
   "source": [
    "driver = GraphDatabase.driver(connectionUrl, auth=(username, password))\n",
    "driver.verify_connectivity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d1b7ed-539d-4d41-a2b8-0969bcc41395",
   "metadata": {
    "id": "44d1b7ed-539d-4d41-a2b8-0969bcc41395"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def run_query(query, params={}):\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query, params)\n",
    "        return pd.DataFrame([r.values() for r in result], columns=result.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228a3a58",
   "metadata": {
    "id": "228a3a58"
   },
   "source": [
    "Before loading the data, create constraints as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66756bab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "id": "66756bab",
    "outputId": "9d74f22e-e5e0-4fbb-eab9-c7981898b9ff"
   },
   "outputs": [],
   "source": [
    "run_query('CREATE CONSTRAINT unique_person_id IF NOT EXISTS FOR (n:Person) REQUIRE (n.id) IS UNIQUE')\n",
    "run_query('CREATE CONSTRAINT unique_position_id IF NOT EXISTS FOR (n:Position) REQUIRE (n.id) IS UNIQUE')\n",
    "run_query('CREATE CONSTRAINT unique_skill_id IF NOT EXISTS FOR (n:Skill) REQUIRE n.id IS UNIQUE')\n",
    "run_query('CREATE CONSTRAINT unique_education_id IF NOT EXISTS FOR (n:Education) REQUIRE n.id IS UNIQUE')\n",
    "run_query('CREATE CONSTRAINT unique_company_id IF NOT EXISTS FOR (n:Company) REQUIRE n.id IS UNIQUE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971bf0b3",
   "metadata": {
    "id": "971bf0b3"
   },
   "source": [
    "Ingest the entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7367ece7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7367ece7",
    "outputId": "1489b01d-22a1-4d00-b2b7-39493d5dbab6"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for e in ent_cyp:\n",
    "    run_query(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f811933",
   "metadata": {
    "id": "0f811933"
   },
   "source": [
    "Ingest relationships now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ff4ad1",
   "metadata": {
    "id": "d9ff4ad1",
    "outputId": "f4bf4e39-4e4a-4c11-c6d8-729e967d3d69"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for r in rel_cyp:\n",
    "    run_query(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfnoXjseXv1M",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfnoXjseXv1M",
    "outputId": "fbc7905c-62d3-4835-c291-de1d2db433e5"
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0f5076-188c-49d4-9df9-914540d45618",
   "metadata": {
    "id": "1d0f5076-188c-49d4-9df9-914540d45618"
   },
   "source": [
    "Your ingested data from the above commands might look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46273cf0-fe01-442f-a4ad-1f9fb4e274b4",
   "metadata": {
    "id": "46273cf0-fe01-442f-a4ad-1f9fb4e274b4"
   },
   "source": [
    "![ingested_data.png](attachment:4d918a28-09df-46ef-92a5-29fe0462f490.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65581a1",
   "metadata": {
    "id": "c65581a1"
   },
   "source": [
    "We got thousands of Resumes in the `data` directory. Let us run a pipeline to ingest only a few of them now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b707904c",
   "metadata": {
    "id": "b707904c"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def run_pipeline(start=0, count=1):\n",
    "    txt_files = glob.glob(\"/content/intelligent-app-google-generativeai-neo4j/notebook/data/*.txt\")[start:count]\n",
    "    print(f\"Running pipeline for {len(txt_files)} files\")\n",
    "    failed_files = process_pipeline(txt_files)\n",
    "    print(failed_files)\n",
    "    return failed_files\n",
    "\n",
    "def process_pipeline(files):\n",
    "    failed_files = []\n",
    "    i = 0\n",
    "    for f in files:\n",
    "        i += 1\n",
    "        try:\n",
    "            with open(f, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                print(f\"  {f}: Reading File No. ({i})\")\n",
    "                data = file.read().rstrip()\n",
    "                text = data\n",
    "                print(f\"    {f}: Extracting Entities & Relationships\")\n",
    "                results = run_extraction(f, text)\n",
    "                print(f\"    {f}: Generating Cypher\")\n",
    "                ent_cyp, rel_cyp = generate_cypher(results)\n",
    "                print(f\"    {f}: Ingesting Entities\")\n",
    "                for e in ent_cyp:\n",
    "                    run_query(e)\n",
    "                print(f\"    {f}: Ingesting Relationships\")\n",
    "                for r in rel_cyp:\n",
    "                    run_query(r)\n",
    "                print(f\"    {f}: Processing DONE\")\n",
    "        except Exception as e:\n",
    "            print(f\"    {f}: Processing Failed with exception {e}\")\n",
    "            failed_files.append(f)\n",
    "    return failed_files\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "def run_extraction(f, text):\n",
    "    start = timer()\n",
    "    prompts = [person_prompt_tpl, postion_prompt_tpl, skill_prompt_tpl, edu_prompt_tpl]\n",
    "    results = {\"entities\": [], \"relationships\": []}\n",
    "    for p in prompts:\n",
    "        _prompt = Template(p).substitute(ctext=text)\n",
    "        _extraction = extract_entities_relationships(_prompt, '')\n",
    "        if 'Answer:\\n' in _extraction:\n",
    "            _extraction = _extraction.split('Answer:\\n ')[1]\n",
    "        if _extraction.strip() == '':\n",
    "            continue\n",
    "        try:\n",
    "            _extraction = json.loads(_extraction.replace(\"\\'\", \"'\"))\n",
    "        except json.JSONDecodeError:\n",
    "            #Temp hack to ignore Skills cut off by token limitation\n",
    "            _extraction = _extraction[:_extraction.rfind(\"}\")+1] + ']}'\n",
    "            _extraction = json.loads(_extraction.replace(\"\\'\", \"'\"))\n",
    "        results[\"entities\"].extend(_extraction[\"entities\"])\n",
    "        if \"relationships\" in _extraction:\n",
    "            results[\"relationships\"].extend(_extraction[\"relationships\"])\n",
    "    person_id = results[\"entities\"][0][\"id\"]\n",
    "    for e in results[\"entities\"][1:]:\n",
    "        if e['label'] == 'Position':\n",
    "            results[\"relationships\"].append(f\"{person_id}|HAS_POSITION|{e['id']}\")\n",
    "        if e['label'] == 'Skill':\n",
    "            results[\"relationships\"].append(f\"{person_id}|HAS_SKILL|{e['id']}\")\n",
    "        if e['label'] == 'Education':\n",
    "            results[\"relationships\"].append(f\"{person_id}|HAS_EDUCATION|{e['id']}\")\n",
    "    end = timer()\n",
    "    elapsed = (end-start)\n",
    "    print(f\"    {f}: Entity Extraction took {elapsed}secs\")\n",
    "    return [results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2feb86-2938-4842-9e7b-f2ba45bada1c",
   "metadata": {
    "id": "fe2feb86-2938-4842-9e7b-f2ba45bada1c"
   },
   "source": [
    "Lets run the pipeline only for the first 100 files. This will only process those 10 files and ingested them to Neo4j. It usually takes around 30-45 minutes for 100 files.\n",
    "\n",
    "In your case, you may need to run the pipeline for 1000s of files inside the `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4b86a7",
   "metadata": {
    "id": "bb4b86a7"
   },
   "outputs": [],
   "source": [
    "%%capture ingestion_output\n",
    "%%time\n",
    "failed_files = run_pipeline(0, 100) # runs ingestion pipeline for files from index 0 to 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9367ffca-53ad-42d7-9ff7-35314759a425",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9367ffca-53ad-42d7-9ff7-35314759a425",
    "outputId": "61cfe825-f15f-4b7f-bc5b-9a58f215de11"
   },
   "outputs": [],
   "source": [
    "ingestion_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653e9c48",
   "metadata": {
    "id": "653e9c48"
   },
   "source": [
    "If processing failed for some files due to API Rate limit, you can retry as below. For token limitation error, it is better to chunk the text and retry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e26a851",
   "metadata": {
    "id": "4e26a851"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "failed_files = process_pipeline(failed_files)\n",
    "failed_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UrvbzyY_X8uP",
   "metadata": {
    "id": "UrvbzyY_X8uP",
    "tags": []
   },
   "source": [
    "## Cypher Generation for Consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c06e57d-72a9-4b86-8b8d-119690895c02",
   "metadata": {
    "id": "7c06e57d-72a9-4b86-8b8d-119690895c02",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Tune the model to generate Cypher (OPTIONAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe379dfb-023f-4664-82e8-7fc8e016ba5d",
   "metadata": {
    "id": "fe379dfb-023f-4664-82e8-7fc8e016ba5d"
   },
   "source": [
    "The Codey family of models perform well for Cypher generation with few-shot prompting. However, they are not tunable at the moment. If you need to tune a model for specific Cypher Generation task, you can consider `text-bison` model we used during the ingestion process above. So, the tuning section below is completely optional.\n",
    "\n",
    "\n",
    "The `text-bison` base model can be tuned to generate more accurate Cypher. Lets see how to adapter tune it. We will try to tune the model with some Cypher statements. The model achieves some Cypher generation capability but could be better. It is suggested to try with at least a few hundred statements. You should aim for more quality training data.\n",
    "\n",
    "The total training time below takes more than an hour. The tuned adapter model is going to stay within your tenant and your training data will not be used to train the base model which is frozen. Tuning runs on GCP's TPU infrastructure that is optimised to run ML workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f785e716-2ba1-4d9f-b949-296c9285a1bc",
   "metadata": {
    "id": "f785e716-2ba1-4d9f-b949-296c9285a1bc"
   },
   "source": [
    "First, let us upload our training set in `jsonl` format to a GCS bucket. We will use this file `finetuning/eng-to-cypher-trng.jsonl` for our fine-tuning. You can take a look over the data there.\n",
    "\n",
    "VertexAI expects you to adhere to this format for each line of the `jsonl` file.\n",
    "```json\n",
    "{\"input_text\": \"MY_INPUT_PROMPT\", \"output_text\": \"CYPHER_QUERY\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a2ede9-643b-4adc-a30f-0f26fb61fd47",
   "metadata": {
    "id": "b9a2ede9-643b-4adc-a30f-0f26fb61fd47"
   },
   "source": [
    "When you got some changes in the training data, ensure that you upload the updated file in a different name than your previous tuning exercises. Because Vertex AI caches data uploaded previously, it skips any file validation and uses the previously uploaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6316e464-5dbf-475d-8507-de07ba0bc0c0",
   "metadata": {
    "id": "6316e464-5dbf-475d-8507-de07ba0bc0c0"
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "bucket_name = project_id + '-genai'\n",
    "client = storage.Client()\n",
    "try:\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "except:\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    bucket.storage_class = 'STANDARD'\n",
    "    bucket = client.create_bucket(bucket)\n",
    "\n",
    "upload_name = f\"finetuning/eng-to-cypher-trng-{timer()}.jsonl\" #this ensures vertexai reloads the file\n",
    "filename = 'finetuning/eng-to-cypher-trng.jsonl'\n",
    "blob = bucket.blob(upload_name)\n",
    "blob.upload_from_filename(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f8cd32-e509-4ec1-bd74-ba6ee9142c3f",
   "metadata": {
    "id": "76f8cd32-e509-4ec1-bd74-ba6ee9142c3f"
   },
   "source": [
    "Let's tune the model for a hundred training steps. When you the below code, the following sequence happens:\n",
    "1. Pipeline Validation\n",
    "2. Dataset Export\n",
    "3. Prompt Validation\n",
    "4. jsonl to tfrecord conversion\n",
    "5. Parameter Composition for Adapter tuning\n",
    "6. LLM Tuning\n",
    "7. Model uploading and\n",
    "8. Endpoint deployment\n",
    "\n",
    "![finetune-seq.png](attachment:2779e9d0-845d-4aea-a38d-4e5f681ba36d.png)\n",
    "\n",
    "This tuning takes approximately 2 hours to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52790af0-83c8-4373-8716-91fcea4b87a7",
   "metadata": {
    "id": "52790af0-83c8-4373-8716-91fcea4b87a7"
   },
   "outputs": [],
   "source": [
    "training_data = 'gs://' + bucket_name + '/' + upload_name\n",
    "train_steps = 100\n",
    "\n",
    "vertexai.init(project=project_id, location=location)\n",
    "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "\n",
    "model.tune_model(\n",
    "  training_data=training_data,\n",
    "  train_steps=train_steps,\n",
    "  tuning_job_location=\"europe-west4\",\n",
    "  tuned_model_location=\"us-central1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b313858-08be-4e62-bd49-c120afac0b44",
   "metadata": {
    "id": "4b313858-08be-4e62-bd49-c120afac0b44"
   },
   "source": [
    "To get the details of the adapter tuned model, run this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92112808-bb7c-460f-b06e-a9c319bbc8fa",
   "metadata": {
    "id": "92112808-bb7c-460f-b06e-a9c319bbc8fa",
    "outputId": "7455e8a7-7783-4a80-987b-bdbcb4931969"
   },
   "outputs": [],
   "source": [
    "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "models = model.list_tuned_model_names()\n",
    "\n",
    "# The first model in the list is the one we just tuned.\n",
    "entity_extraction_tuned_model = models[0]\n",
    "entity_extraction_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9f2895-c3c4-401c-a4d6-cf1b80a2050b",
   "metadata": {
    "id": "5e9f2895-c3c4-401c-a4d6-cf1b80a2050b"
   },
   "outputs": [],
   "source": [
    "def english_to_cypher_text_bison(prompt, tuned_model_name = ''):\n",
    "    try:\n",
    "        res = run_text_model(project_id, \"text-bison@001\", 0.1, 1024, 0.95, 40, prompt, location, tuned_model_name)\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a113928b-b83d-4f41-833b-a92e851cca3a",
   "metadata": {
    "id": "a113928b-b83d-4f41-833b-a92e851cca3a",
    "tags": []
   },
   "source": [
    "### Generate Cypher\n",
    "If you are not tuning the `text-bison` model for Cypher generation, you can consider the `code-bison` model from the Codey Models family. Let us see how to use it for Cypher generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1148c7-b0ac-44ed-a8af-d557c10fbcd6",
   "metadata": {
    "id": "5f1148c7-b0ac-44ed-a8af-d557c10fbcd6"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform.gapic.schema import predict\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "\n",
    "def generate_code(\n",
    "    api_endpoint: str,\n",
    "    endpoint: str,\n",
    "    input: str,\n",
    "    parameters: str,\n",
    "    location: str = \"us-central1\",\n",
    "):\n",
    "  # The AI Platform services require regional API endpoints.\n",
    "  client_options = {\"api_endpoint\": api_endpoint}\n",
    "  # Initialize client that will be used to create and send requests.\n",
    "  # This client only needs to be created once, and can be reused for multiple requests.\n",
    "  client = aiplatform.gapic.PredictionServiceClient(\n",
    "      client_options=client_options\n",
    "  )\n",
    "  instance_dict = input\n",
    "  instance = json_format.ParseDict(instance_dict, Value())\n",
    "  instances = [instance]\n",
    "  parameters_dict = parameters\n",
    "  parameters = json_format.ParseDict(parameters_dict, Value())\n",
    "  response = client.predict(\n",
    "      endpoint=endpoint, instances=instances, parameters=parameters\n",
    "  )\n",
    "  predictions = response.predictions\n",
    "  return predictions[0][\"content\"]\n",
    "\n",
    "def english_to_cypher_code_bison(prompt):\n",
    "    try:\n",
    "        res = generate_code(\"us-central1-aiplatform.googleapis.com\", \"projects/workshop-trvlk/locations/us-central1/publishers/google/models/code-bison@001\", {\n",
    "                  \"prefix\": prompt\n",
    "                }, {\"temperature\": 0, \"maxOutputTokens\": 2048}, \"us-central1\")\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d2f78f-74f7-4ef2-bb65-ed9c83d69f95",
   "metadata": {
    "id": "f1d2f78f-74f7-4ef2-bb65-ed9c83d69f95"
   },
   "source": [
    "We have to create a prompt template that clearly states what schema to use, what kind of Cypher to generate and how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cfc38e-695d-4f51-8028-f44d24597158",
   "metadata": {
    "id": "55cfc38e-695d-4f51-8028-f44d24597158"
   },
   "outputs": [],
   "source": [
    "samples = \"\"\"\n",
    "Question: How many expert java developers attend more than one universities?\n",
    "Answer: MATCH (p:Person)-[:HAS_SKILL]->(s:Skill), (p)-[:HAS_EDUCATION]->(e1:Education), (p)-[:HAS_EDUCATION]->(e2:Education) WHERE toLower(s.name) CONTAINS 'java' AND toLower(s.level) CONTAINS 'expert' AND e1.university <> e2.university RETURN COUNT(DISTINCT p)\n",
    "\n",
    "Question: Where do most candidates get educated?\n",
    "Answer: MATCH (p:Person)-[:HAS_EDUCATION]->(e:Education) RETURN e.university, count(e.university) as alumni ORDER BY alumni DESC LIMIT 1\n",
    "\n",
    "Question: How many people have worked as a Data Scientist in San Francisco?\n",
    "Answer: MATCH (p:Person)-[:HAS_POSITION]->(pos:Position) WHERE toLower(pos.title) CONTAINS 'data scientist' AND toLower(pos.location) CONTAINS 'san francisco' RETURN COUNT(p)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13794151-c535-4603-b825-b7d6adb51a12",
   "metadata": {
    "id": "13794151-c535-4603-b825-b7d6adb51a12"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are an expert Neo4j Cypher translator who understands the question in english and convert to Cypher strictly based on the Neo4j Schema provided and following the instructions below:\n",
    "1. Generate Cypher query compatible ONLY for Neo4j Version 5\n",
    "2. Do not use EXISTS, SIZE keywords in the cypher. Use alias when using the WITH keyword\n",
    "3. Use only Nodes and relationships mentioned in the schema\n",
    "4. Always enclose the Cypher output inside 3 backticks\n",
    "5. Always do a case-insensitive and fuzzy search for any properties related search. Eg: to search for a Company name use `toLower(c.name) contains 'neo4j'`\n",
    "6. Candidate node is synonymous to Person\n",
    "7. Always use aliases to refer the node in the query\n",
    "8. Cypher is NOT SQL. So, do not mix and match the syntaxes\n",
    "Schema:\n",
    "(:Person {label: 'Person', id: string, role: string, description: string})-[:HAS_POSITION {}]->(:Position {label: 'Position', id: string, title: string, location: string, startDate: string, endDate: string, url: string})\n",
    "(:Position {label: 'Position', id: string, title: string, location: string, startDate: string, endDate: string, url: string})-[:AT_COMPANY {}]->(:Company {label:'Company', id: string, name: string})\n",
    "(:Person {label: 'Person',id: string, role: string, description: string})-[:HAS_SKILL {}]->(:Skill {label:'Skill', id: string,name: string,level: string})\n",
    "(:Person {label: 'Person',id: string, role: string, description: string})-[:HAS_EDUCATION {}]->(:Education {label:'Education', id: string, degree: string, university: string, graduationDate: string, score: string, url: string})\n",
    "Samples:\n",
    "$samples\n",
    "Question: $question\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EMZysiC9YsqC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "EMZysiC9YsqC",
    "outputId": "806434a6-9dcf-440d-ec15-af3eb8d1b53a"
   },
   "outputs": [],
   "source": [
    "from string import Template\n",
    "que = 'How many are knowledgable on all of - java, python, javascript, security?'\n",
    "_prompt = Template(prompt).substitute(samples=samples, question=que)\n",
    "\n",
    "cypher = english_to_cypher_code_bison(_prompt) #for text-bison use: english_to_cypher_text_bison(_prompt, entity_extraction_tuned_model)\n",
    "if 'Answer:\\n ' in cypher:\n",
    "    cypher = cypher.split('Answer:\\n ')[1]\n",
    "cypher = cypher.replace('\\n', ' ')\n",
    "cypher\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ac31d0-ca59-47cc-81be-ebbd121d86d8",
   "metadata": {
    "id": "90ac31d0-ca59-47cc-81be-ebbd121d86d8",
    "tags": []
   },
   "source": [
    "## Talent Finder Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9212ac86-6c96-4640-a545-c9655e8d52d8",
   "metadata": {
    "id": "9212ac86-6c96-4640-a545-c9655e8d52d8"
   },
   "source": [
    "You can also create a chatbot that can help our interaction with Neo4j using English.\n",
    "\n",
    "Both Vertex AI and Neo4j support LangChain.  We will be using LangChain to quickly build a chatbot that converts English to Cypher and then executes it on Neo4j.  This is augmented using generative AI before sending the response to the user.  This makes graph consumption easier for non-cypher experts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364e1171-1c1f-4fff-8cf3-58426e76f030",
   "metadata": {
    "id": "364e1171-1c1f-4fff-8cf3-58426e76f030"
   },
   "source": [
    "The diagram below shows how Neo4j and Vertex AI will interact using LangChain.\n",
    "\n",
    "![langchain-neo4j.png](attachment:3c17193b-0097-47b6-bd7c-8f8659aa0e2f.png)\n",
    "\n",
    "First we have to create Neo4jGraph and VertexLLM Connection objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323de133-3779-4466-aea4-3ab66c262002",
   "metadata": {
    "id": "323de133-3779-4466-aea4-3ab66c262002"
   },
   "source": [
    "**Currently, VertexAI Langchain does not support Codey Models. This code below is a custom wrapper for `code-bison` model. You can remove this code once, Langchain support VertexAI Codey models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf11973-e610-4d7b-9217-8b4001132cbc",
   "metadata": {
    "id": "3bf11973-e610-4d7b-9217-8b4001132cbc"
   },
   "outputs": [],
   "source": [
    "\"\"\"Wrapper around Google VertexAI Codey models.\"\"\"\n",
    "from typing import TYPE_CHECKING, Any, Dict, List, Optional\n",
    "\n",
    "from pydantic import BaseModel, root_validator\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.llms.utils import enforce_stop_tokens\n",
    "from langchain.utilities.vertexai import (\n",
    "    init_vertexai,\n",
    "    raise_vertex_import_error,\n",
    ")\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform.gapic.schema import predict\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from vertexai.language_models._language_models import _LanguageModel\n",
    "\n",
    "\n",
    "class _VertexAICommon(BaseModel):\n",
    "    client: \"_LanguageModel\" = None  #: :meta private:\n",
    "    model_name: str\n",
    "    \"Model name to use.\"\n",
    "    temperature: float = 0.0\n",
    "    \"Sampling temperature, it controls the degree of randomness in token selection.\"\n",
    "    max_output_tokens: int = 128\n",
    "    \"Token limit determines the maximum amount of text output from one prompt.\"\n",
    "    top_p: float = 0.95\n",
    "    \"Tokens are selected from most probable to least until the sum of their \"\n",
    "    \"probabilities equals the top-p value.\"\n",
    "    top_k: int = 40\n",
    "    \"How the model selects tokens for output, the next token is selected from \"\n",
    "    \"among the top-k most probable tokens.\"\n",
    "    stop: Optional[List[str]] = None\n",
    "    \"Optional list of stop words to use when generating.\"\n",
    "    project: Optional[str] = None\n",
    "    \"The default GCP project to use when making Vertex API calls.\"\n",
    "    location: str = \"us-central1\"\n",
    "    \"The default location to use when making API calls.\"\n",
    "    credentials: Any = None\n",
    "    \"The default custom credentials (google.auth.credentials.Credentials) to use \"\n",
    "    \"when making API calls. If not provided, credentials will be ascertained from \"\n",
    "    \"the environment.\"\n",
    "\n",
    "    @property\n",
    "    def _default_params(self) -> Dict[str, Any]:\n",
    "        base_params = {\n",
    "            \"temperature\": self.temperature,\n",
    "            \"max_output_tokens\": self.max_output_tokens,\n",
    "            \"top_k\": self.top_k,\n",
    "            \"top_p\": self.top_p,\n",
    "        }\n",
    "        return {**base_params}\n",
    "\n",
    "    def _predict(\n",
    "        self, prompt: str, stop: Optional[List[str]] = None, **kwargs: Any\n",
    "    ) -> str:\n",
    "        instance_dict = {\"prefix\": prompt}\n",
    "        instance = json_format.ParseDict(instance_dict, Value())\n",
    "        instances = [instance]\n",
    "        parameters_dict = {\n",
    "            \"temperature\": self.temperature,\n",
    "            \"max_output_tokens\": self.max_output_tokens,\n",
    "            \"top_k\": self.top_k,\n",
    "            \"top_p\": self.top_p,\n",
    "        }\n",
    "        parameters = json_format.ParseDict(parameters_dict, Value())\n",
    "        res = self.client.predict(\n",
    "          endpoint=self.model_name, instances=instances, parameters=parameters\n",
    "        )\n",
    "        return self._enforce_stop_words(res.predictions[0][\"content\"], stop)\n",
    "\n",
    "    def _enforce_stop_words(self, text: str, stop: Optional[List[str]] = None) -> str:\n",
    "        if stop is None and self.stop is not None:\n",
    "            stop = self.stop\n",
    "        if stop:\n",
    "            return enforce_stop_tokens(text, stop)\n",
    "        return text\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"vertexai\"\n",
    "\n",
    "    @classmethod\n",
    "    def _try_init_vertexai(cls, values: Dict) -> None:\n",
    "        allowed_params = [\"project\", \"location\", \"credentials\"]\n",
    "        params = {k: v for k, v in values.items() if k in allowed_params}\n",
    "        init_vertexai(**params)\n",
    "        return None\n",
    "\n",
    "\n",
    "class VertexAICode(_VertexAICommon, LLM):\n",
    "    \"\"\"Wrapper around Google Vertex AI large language models.\"\"\"\n",
    "\n",
    "    model_name: str = \"projects/workshop-trvlk/locations/us-central1/publishers/google/models/code-bison@001\"\n",
    "    tuned_model_name: Optional[str] = None\n",
    "    \"The name of a tuned model, if it's provided, model_name is ignored.\"\n",
    "\n",
    "    @root_validator()\n",
    "    def validate_environment(cls, values: Dict) -> Dict:\n",
    "        \"\"\"Validate that the python package exists in environment.\"\"\"\n",
    "        cls._try_init_vertexai(values)\n",
    "        try:\n",
    "            from vertexai.preview.language_models import TextGenerationModel\n",
    "        except ImportError:\n",
    "            raise_vertex_import_error()\n",
    "        client_options = {\"api_endpoint\": \"us-central1-aiplatform.googleapis.com\"}\n",
    "        # Initialize client that will be used to create and send requests.\n",
    "        # This client only needs to be created once, and can be reused for multiple requests.\n",
    "        values[\"client\"] = aiplatform.gapic.PredictionServiceClient(\n",
    "          client_options=client_options\n",
    "        )\n",
    "        return values\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Call Vertex model to get predictions based on the prompt.\n",
    "        Args:\n",
    "            prompt: The prompt to pass into the model.\n",
    "            stop: A list of stop words (optional).\n",
    "            run_manager: A Callbackmanager for LLM run, optional.\n",
    "        Returns:\n",
    "            The string generated by the model.\n",
    "        \"\"\"\n",
    "        return self._predict(prompt, stop, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5945f4b2-fae8-4f63-b44e-340409f4d8ca",
   "metadata": {
    "id": "5945f4b2-fae8-4f63-b44e-340409f4d8ca"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain.graphs import Neo4jGraph\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "CYPHER_GENERATION_TEMPLATE = \"\"\"You are an expert Neo4j Cypher translator who understands the question in english and convert to Cypher strictly based on the Neo4j Schema provided and following the instructions below:\n",
    "1. Generate Cypher query compatible ONLY for Neo4j Version 5\n",
    "2. Do not use EXISTS, SIZE keywords in the cypher. Use alias when using the WITH keyword\n",
    "3. Use only Nodes and relationships mentioned in the schema\n",
    "4. Always enclose the Cypher output inside 3 backticks\n",
    "5. Always do a case-insensitive and fuzzy search for any properties related search. Eg: to search for a Company name use `toLower(c.name) contains 'neo4j'`\n",
    "6. Candidate node is synonymous to Person\n",
    "7. Always use aliases to refer the node in the query\n",
    "8. Cypher is NOT SQL. So, do not mix and match the syntaxes\n",
    "Schema:\n",
    "{schema}\n",
    "Samples:\n",
    "Question: How many expert java developers attend more than one universities?\n",
    "Answer: MATCH (p:Person)-[:HAS_SKILL]->(s:Skill), (p)-[:HAS_EDUCATION]->(e1:Education), (p)-[:HAS_EDUCATION]->(e2:Education) WHERE toLower(s.name) CONTAINS 'java' AND toLower(s.level) CONTAINS 'expert' AND e1.university <> e2.university RETURN COUNT(DISTINCT p)\n",
    "Question: Where do most candidates get educated?\n",
    "Answer: MATCH (p:Person)-[:HAS_EDUCATION]->(e:Education) RETURN e.university, count(e.university) as alumni ORDER BY alumni DESC LIMIT 1\n",
    "Question: How many people have worked as a Data Scientist in San Francisco?\n",
    "Answer: MATCH (p:Person)-[:HAS_POSITION]->(pos:Position) WHERE toLower(pos.title) CONTAINS 'data scientist' AND toLower(pos.location) CONTAINS 'san francisco' RETURN COUNT(p)\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "CYPHER_GENERATION_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"schema\", \"question\"], template=CYPHER_GENERATION_TEMPLATE\n",
    ")\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=connectionUrl,\n",
    "    username='neo4j',\n",
    "    password=password\n",
    ")\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    VertexAICode(model_name='projects/workshop-trvlk/locations/us-central1/publishers/google/models/code-bison@001',\n",
    "            max_output_tokens=2048,\n",
    "            temperature=0,\n",
    "            top_p=0.95,\n",
    "            top_k=0.40), graph=graph, verbose=True,\n",
    "            cypher_prompt=CYPHER_GENERATION_PROMPT,\n",
    "    return_intermediate_steps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8795e6a9-fa92-4a5f-8cc5-44a20d89d142",
   "metadata": {
    "id": "8795e6a9-fa92-4a5f-8cc5-44a20d89d142"
   },
   "source": [
    "That's it! You can run the agent now. Simply provide the command in English. You get Cypher as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bacd47c-b9dc-4da3-9344-149b23665cb2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bacd47c-b9dc-4da3-9344-149b23665cb2",
    "outputId": "bf1a7c3a-2a76-4355-b8a1-7137db819640"
   },
   "outputs": [],
   "source": [
    "r = chain(\"\"\"How many experts do we have on python\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc336003-88d1-40ad-a634-94ccfe77ef8b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cc336003-88d1-40ad-a634-94ccfe77ef8b",
    "outputId": "d06aed1b-32d9-44ee-8050-314afefe5e8f"
   },
   "outputs": [],
   "source": [
    "print(f\"Intermediate steps: {r['intermediate_steps']}\")\n",
    "print(f\"Final answer: {r['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0f2c16-16cb-41f0-b50c-a8047c1a1517",
   "metadata": {
    "id": "4f0f2c16-16cb-41f0-b50c-a8047c1a1517"
   },
   "source": [
    "### Chatbot!\n",
    "Time to build a chatbot. We will be using Gradio to quickly try out our chatbot that uses a base model. Once VertexLLM is integrated into Langchain, you will get support for adapter tuned model as well.\n",
    "\n",
    "Running the code below will render a chat widget. You can view the Cypher generated for your input below this rendering.\n",
    "\n",
    "Note - Due to quota limitations, you might see errors while submitting the input. You need to wait a while in between your queries.\n",
    "\n",
    "Some sample questions to try out:\n",
    "\n",
    "1. How many experts do we have on MS Word?\n",
    "5. Who went to most number of universities and how many did they go to?\n",
    "6. Where do most candidates get educated?\n",
    "7. How many people know Delphi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a060728-90a7-48fd-a197-ecd099b887bc",
   "metadata": {
    "id": "2a060728-90a7-48fd-a197-ecd099b887bc",
    "outputId": "4269b20a-a051-4a43-a2f5-c136c13c8453"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key = \"chat_history\", return_messages = True)\n",
    "llm = VertexAICode(model_name='projects/neo4jbusinessdev/locations/us-central1/publishers/google/models/code-bison@001',\n",
    "            max_output_tokens=2048,\n",
    "            temperature=0,\n",
    "            top_p=0.95,\n",
    "            top_k=0.40)\n",
    "agent_chain = chain\n",
    "def chat_response(input_text):\n",
    "    response = agent_chain.run(input_text)\n",
    "    return response\n",
    "\n",
    "interface = gr.Interface(fn = chat_response, inputs = \"text\", outputs = \"text\",\n",
    "                         description = \"Talent Finder Chatbot\")\n",
    "\n",
    "interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3f9ec1-b87b-4eee-9462-2a4c45218d25",
   "metadata": {
    "id": "fc3f9ec1-b87b-4eee-9462-2a4c45218d25"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
